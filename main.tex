\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{left = 1.5cm, right = 1.5cm, top = 1.5cm, bottom = 2cm}
%\setcounter{tocdepth}{4}
\usepackage[onehalfspacing]{setspace} 
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{array}
\usepackage[
   colorlinks,% Links ohne Umrandungen in zu wählender Farbe
   linkcolor=black,   % Farbe interner Verweise
   filecolor=black,   % Farbe externer Verweise
   citecolor=black% Farbe von Zitaten
]{hyperref}
\usepackage{float} 
\usepackage{ragged2e} %%blocksatz \justifing
\usepackage{minted}
\usepackage[ngerman]{babel}
\usepackage[babel,german=guillemets]{csquotes}
\usepackage[backend=biber, style=apa]{biblatex}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, positioning}
\usepackage{stmaryrd}
\usepackage{enumitem}
\usepackage{pdfpages}
\DeclareLanguageMapping{german}{german-apa}
\newcommand{\showfontsize}{{\f@size pt}}
\addbibresource{literatur.bib}
\setlength{\bibitemsep}{1em}
%\setlist[itemize,1]{label=•, itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft}
%\setlist[itemize,2]{label=•, itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=1.5em}
%\setlist[itemize,3]{label=•, itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=2.5em}


\begin{document}
\justifying

\begin{center}
\LARGE Statistik für Data Science\\[1em]
\large Flurina Sophie Baumbach\\[0.5em]
\normalsize Sep 15, 2025
\end{center}

\renewcommand{\contentsname}{Inhaltsverzeichnis}


\tableofcontents\newpage

\newpage

\section{Daten}

\subsection{Datentypen \& Messniveaus}
Statistik hängt vom Datentyp ab - Unterschiedliche Methoden für verschiedene Skalen

\subsubsection{Nominalskala}
\textbf{kategorisch, keine Ordnung}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Niedrigster Informationsgehalt
    \item Nominalskalierte Daten lassen sich weder in eine logische Reihenfolge sortieren, noch quantitativ differenzieren
    \item Merkmalsausprägungen zweier Merkmale lassen sich also lediglich in Gleichheit oder Ungleichheit unterscheiden (A $\not=$ B)
    \item Werte sind reine Namen / Kategorien, \textbf{keine Reihenfolge, keine Abstände}
    \item Beispiele: Geschlecht, Postleitzahl, HTTP-Statuscode
\end{itemize}

\subsubsection{Ordinalskala}
\textbf{kategorisch, mit Ordnung}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Ordnet Variablen mit Ausprägungen in eine klare Rangfolge
    \item Die Abstände zwischen den einzelnen Rängen sind nicht interpretierbar, da sie nicht quantifiziert sind
    \item Beispiele: Schulnoten, Star Ratings
\end{itemize}

\subsubsection{Intervallskala}
\textbf{metrisch, ohne echten Nullpunkt}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Es lassen sich Reihenfolgen und quantifizierbare Abstände bilden
    \item Sie liegt beim Skalenniveau ebenfalls über der Nominalskala und der Ordinalskala
    \item kein absoluter/natürlicher Nullpunkt (Verhältnisse („doppelt so viel“) ergeben keinen Sinn)
    \item Beispiele: Temperatur in Celsius, Zeit
\end{itemize}

\subsubsection{Ratioskala}
\textbf{metrisch, mit absolutem Nullpunkt}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Es lassen sich sowohl Reihenfolgen als auch quantifizierbare Abstände bilden
    \item Alle Eigenschaften der Intervallskala + natürlicher Nullpunkt $\rightarrow$ Verhältnisse sind sinnvoll
    \item Beispiele: Alter in Jahren, RAM-Größe, Gewicht, Einkommen
\end{itemize}

\subsection{Bias in Daten}
\textbf{Bias = systematische Verzerrung in Daten
oder Analyse}

\subsubsection{Sampling Bias}
Entsteht, wenn die Stichprobe nicht repräsentativ für die gesamte Population ist

\subsubsection{Survivorship Bias}
Entsteht, wenn man nur die „Überlebenden“ einer Gruppe betrachtet und dadurch die Analyse verzerrt wird\\
$\rightarrow$ Man übersieht die Fälle, die nicht mehr da sind – und genau die enthalten oft die wichtigste Information

\subsubsection{Confirmation Bias}
Man sucht nur nach Informationen, die die eigene Hypothese oder Erwartung bestätigen und widersprechende Daten werden ignoriert oder abgewertet

\subsubsection{Publication Bias}
Es werden nur „positive“ oder „signifikante“ Ergebnisse veröffentlicht, während neutrale oder negative Resultate in der Schublade verschwinden $\Rightarrow$ verzerrtes Bild in der Wissenschaft oder Praxis

\subsubsection{Measurement Bias}
Entsteht, wenn die Messungen selbst systematisch fehlerhaft oder verzerrt sind $\rightarrow$ Es liegt also nicht an der Stichprobe oder Veröffentlichung, sondern an der Art und Weise, wie Daten erfasst werden

\subsection{Missing Values}

\subsubsection{MCAR – Completely at
Random}
M\textbf{C}AR: \textbf{C}ompletely = Chaos, reiner Zufall
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Fehlende Werte treten \textbf{völlig zufällig} auf
    \item Es gibt keinen Zusammenhang mit beobachteten Variablen oder mit dem wahren Wert selbst
    \item Kein Bias $\rightarrow$ die Daten bleiben repräsentativ (Man darf die fehlenden Werte ignorieren oder simpel (ggf. durch den Median) ersetzen)
\end{itemize}

\subsubsection{MAR – At Random}
M\textbf{AR}: \textbf{A}t \textbf{R}andom" = Abhängig von Anderen (beobachteten) Variablen
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Fehlende Werte hängen \textbf{nicht vom wahren Wert selbst} ab, sondern \textbf{von anderen beobachteten Variablen}
    \item Das Fehlen ist also nicht völlig zufällig, sondern erklärbar durch bekannte Informationen
    \item MAR-Daten sind modellierbar $\rightarrow$ man kann fortgeschrittene Imputationsmethoden nutzen, z.B. gruppenweise Mittelwert/Median, MICE (Multiple Imputation by Chained Equations) oder KNN-Imputation (K-Nearest Neighbors)
\end{itemize}

\subsubsection{MNAR – Not at Random}
M\textbf{N}AR: \textbf{N}ot Random = Nicht zufällig, systematisches Problem
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Das Fehlen hängt direkt vom wahren Wert selbst ab
    \item Menschen oder Systeme „verstecken“ bestimmte Werte systematisch $\rightarrow$ also nicht zufällig, nicht durch andere Variablen erklärbar, sondern abhängig von der fehlenden Größe selbst
    \item Klassische Imputation (Mittelwert, Median) führt zu Verzerrungen $\rightarrow$ man braucht spezielle Modelle oder Expertenwissen / Domain Knowledge
\end{itemize}

\subsubsection{Lösungsansätze}
\begin{tabular}{|c|p{4cm}|p{5cm}|p{5cm}|}
\hline
\textbf{\%} & \textbf{MCAR} & \textbf{MAR} & \textbf{MNAR} \\
\hline
$<$ 5\% & Löschen OK & Gruppenweise Imputation & Expertenwissen (z.\,B. Arzt) \\
\hline
5--20\% & Multiple Imputation & MICE / KNN & Selection Models (z.\,B. Heckman) \\
\hline
$>$ 20\% & Multiple Imputation & MICE + Sensitivity & Explizite Modellierung (Warum?) \\
\hline
\end{tabular}

\subsubsection{Strategie - Median / Mean Imputation}
Fehlende Werte werden durch den Mittelwert oder Median der vorhandenen Werte ersetzt\\
$\rightarrow$ Vorteile: Sehr einfach und schnell, gut, wenn nur wenige Werte fehlen (< 5 \%) - wird bei MCAR verwendet\\
$\rightarrow$ Nachteile: Unterschätzt die Varianz $\rightarrow$ Daten wirken zu gleichmäßig, kann Zusammenhänge zwischen Variablen verzerren und ignoriert Unsicherheit (alle fehlenden bekommen denselben Wert)

\subsubsection{Strategie - Gruppenweise Imputation}
Fehlende Werte werden \textbf{nicht global}, sondern \textbf{innerhalb von Untergruppen} ersetzt $\rightarrow$ Man teilt die Daten nach einer Gruppenvariablen (z. B. Geschlecht, Region, Altersklasse) auf und imputiert gruppenweise den Median oder Mittelwert\\
$\rightarrow$ Vorteile: Berücksichtigt strukturelle Unterschiede zwischen Gruppen, weniger Verzerrung als globale Mean/Median-Imputation, einfach anzuwenden, wenn eine gute Gruppenvariable vorhanden ist.\\
$\rightarrow$ Nachteile: Innerhalb der Gruppen wird die Varianz trotzdem unterschätzt, man braucht eine vollständige und sinnvolle Gruppenvariable (z. B. „Geschlecht“ darf nicht selbst viele Missing Values haben), funktioniert nur gut, wenn Gruppen groß genug sind

\subsubsection{Strategie - Modellbasierte Imputation}
Fehlende Werte werden mit Hilfe eines Vorhersagemodells geschätzt, das aus den vorhandenen Daten gelernt wird - statt einfach Median oder Mittelwert einzusetzen, nutzt man \textbf{Zusammenhänge zwischen Variablen}\\
$\rightarrow$ Vorteile: Nutzt komplexe Zusammenhänge $\rightarrow$ realistischere Schätzungen, Erhält Varianz besser als Mean/Median-Imputation, kann auch nicht-lineare Muster erfassen\\
$\rightarrow$ Nachteile: Rechenaufwendig, komplex, Gefahr von Overfitting, Qualität hängt stark von den Trainingsdaten ab

\subsubsection{Vergleichstabelle Imputationsmethoden}
\begin{tabular}{|l|c|c|c|l|}
\hline
\textbf{Methode} & \textbf{Komplexität} & \textbf{Varianz} & \textbf{Beziehungen} & \textbf{Wann nutzen?} \\
\hline
Mean/Median & $*$ & \ding{55} Unterschätzt & \ding{55} Ignoriert & MCAR, < 5\% fehlend \\
\hline
Gruppenweise & $**$ & \ding{108} Teilweise & \ding{51} Innerhalb Gruppen & MAR mit klaren Gruppen \\
\hline
KNN & $***$ & \ding{51} Erhält & \ding{51} Lokal & Lokale Cluster \\
\hline
MICE & $****$ & \ding{51} Erhält & \ding{51} Global & MAR, viele Variablen \\
\hline
Deep Learning & $*****$ & \ding{51} Erhält & \ding{51} Komplex & Große Datensätze \\
\hline
\end{tabular}

\section{EDA - Exploratory Data Analysis}
Muster erkennen, Fehler finden, Hypothesen generieren

\subsection{Scatterplot}
\noindent
\begin{minipage}{0.48\textwidth}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, 
                labelsep=0.5em, leftmargin=*, align=parleft]
    \item Stellt die Beziehung zwischen zwei metrischen Variablen in einem Koordinatensystem dar
    \item Jeder Datenpunkt = ein Beobachtungspaar (x,y)
    \item Grundlage für lineare Regression
\end{itemize}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Scatterplot.png}
    \vspace{-0.5em}
\end{minipage}

\section{Lagekennzahlen}
Lagekennzahlen sollten \textbf{immer zusammen mit Streuungsmaßen} berichtet werden, da dieselbe Lage unterschiedliche Streuungen haben kann.\\ $\rightarrow$ Beispiel: Median = 40 ist nicht informativ, wenn die Spannweite 39–41 oder 10–1000 sein könnte.

\subsection{ECDF inverse zu Quantile}
\textbf{ECDF} = Empirical Cumulative Distribution Function (empirische Verteilungsfunktion)\\
Sie zeigt für jeden Wert x, welcher Anteil der Daten kleiner oder gleich x ist.\\
$F(x) = \frac{\text{Anzahl der Werte } \leq x}{n}$, steigt monoton von 0 bis 1\\
\textbf{Quantile} sind die \textbf{inverse Sichtweise}:\\
Bei gegebenem Anteil p (z. B. 0.25, 0.5, 0.75) $\rightarrow$ finde den Wert Q\_p, der mindestens p $\cdot$ 100\% der Daten abdeckt.\\
\noindent\fbox{%
  \parbox{0.26\linewidth}{%
    ECDF: Zahl $\;\to\;$ Prozent\\
    Quantil: Prozent $\;\to\;$ Zahl
  }%
}

\subsection{Mittelwert vs. Median bei Schiefe}
\textbf{Rechts-Schiefe} (positiv skewed): Verteilung hat einen langen rechten „Tail“ (z. B. Einkommen, Wartezeiten)\\
$\rightarrow$ \textbf{Mittelwert} (Mean) liegt \textbf{größer als der Median}, weil er von den hohen Extremwerten nach rechts „gezogen“ wird \\
$\rightarrow$ Median bleibt robust und zeigt das „typische“ Zentrum
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/Bildschirmfoto 2025-09-29 um 14.41.36.png}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Typ} & \textbf{Lage der Kennwerte} & \textbf{Beschreibung} \\
\midrule
Symmetrisch & $\mathrm{Mean}=\mathrm{Median}=\mathrm{Mode}$ & gleichmäßige Verteilung \\
Linksschief (negativ) & $\mathrm{Mean}<\mathrm{Median}<\mathrm{Mode}$ & langer linker Tail \\
Rechtsschief (positiv) & $\mathrm{Mode}<\mathrm{Median}<\mathrm{Mean}$ & langer rechter Tail \\
\bottomrule
\end{tabular}
\end{table}

\section{Streuungskennzahlen}
\textbf{Lagekennzahlen} beschreiben nur das Zentrum\\
\textbf{Streuungskennzahlen} erfassen, wie stark die Daten um dieses Zentrum schwanken $\rightarrow$ „Breite“ oder „Variabilität“ der Verteilung\\

\subsection{Streuungs-Profil als Routine}
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Missing prüfen
    \item Lage robust (Median, IQR), dann klassisch (Mittelwert)
    \item Streuung robust (IQR, MAD), dann klassisch (Varianz, Standardabweichung)
    \item Ausreißerprüfung ankündigen
\end{enumerate}

\section{Ausreisser erkennen und behandeln}

\subsection{Einfluss von Ausreißern}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Mean \& SD kippen sehr schnell}: sie steigen stark an, wenn ein Extremwert hinzukommt
    \item \textbf{Robuste Kennzahlen}: Median, IQR, MAD $\rightarrow$ deshalb immer zuerst berichten, dann erst Ausreißer analysieren
\end{itemize}

\subsection{Klassischer Z-Score}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Misst, wie viele Standardabweichungen ein Wert vom Mittelwert entfernt ist
    \item $z_i = \frac{x_i - \bar{x}}{s}$
    \item Typische Schwelle: $|z| > 3 \rightarrow$ Ausreißer
    \item Nur sinnvoll bei (nahezu) normalverteilten Daten, sonst irreführend
\end{itemize}
\begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{img/Bildschirmfoto 2025-09-29 um 17.44.48.png}
\end{figure}
    
\subsection{Tukey-Fences (Boxplot-Regel)}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Auf Basis des IQR: $\text{Untergrenze} = Q_1 - 1.5 \cdot IQR,\quad \text{Obergrenze} = Q_3 + 1.5 \cdot IQR$
    \item Werte außerhalb gelten als Ausreißer-Kandidaten
    \item Vorteil: \textbf{robust, keine Verteilungsannahme}
    \item Standard in Boxplots
\end{itemize}
\begin{figure}[H]
        \centering
    \includegraphics[width=0.75\linewidth]{img/Bildschirmfoto 2025-09-29 um 17.44.54.png}
\end{figure}

\subsection{Modifizierter Z-Score mit MAD}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Robust gegen Schiefe \& Heavy Tails
    \item $M_i = \frac{0.6745 (x_i - \tilde{x})}{MAD}$
    \item Typische Schwelle: $|M| > 3.5$ → Ausreißer
    \item Sehr geeignet bei nicht-normalen Verteilungen
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/Bildschirmfoto 2025-09-29 um 17.45.01.png}
\end{figure}

\subsection{Strategien im Umgang}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Markieren \& berichten} (für Transparenz, nicht löschen)
    \item \textbf{Winsorisieren}: Extremwerte auf Grenzwert setzen (deckeln)
    \item \textbf{Trimmen}: Ränder entfernen (z. B. unterstes und oberstes 5 \%)
    \item Immer \textbf{Entscheidung dokumentieren} (Regel, Schwelle, Datum, Variable, Datenversion)
\end{itemize}

\subsection{Heavy Tails vs. echte Ausreißer}
\noindent
\begin{minipage}{0.48\textwidth}
\textbf{Heavy Tails} = Verteilungen mit vielen großen Werten (z.B. Einkommen, Versicherungsfälle). \\$\rightarrow$ nicht automatisch Fehler
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\textbf{Echte Ausreißer} = Datenfehler (Messungen, falsche Einheit etc.)
\end{minipage}
\\ \\
Diagnose nur mit Kombination: \textbf{Kennzahlen + Plots (Histogramm, KDE, ECDF, QQ)}

\subsection{Pipeline für konsistentes Handling}
Feste Reihenfolge, um Ad-hoc-Fehler zu vermeiden
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Kennzahlen berechnen (Mean, Median, SD, IQR, MAD)
    \item Plots prüfen (Boxplot, Histogramm, KDE, QQ-Plot)
    \item Regel anwenden (Z-Score, Tukey, mod. Z)
    \item Entscheidung dokumentieren (auch Varianten mit/ohne Ausreißer)
\end{itemize}
\textbf{Zählen $\rightarrow$ Schauen $\rightarrow$ Handeln $\rightarrow$ Dokumentieren}

\section{Histogramm und KDE}

\subsection{Bin-Wahl}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Zu wenige Bins} $\rightarrow$ glatte, aber irreführende Form (wichtige Details verschwinden)
    \item \textbf{Zu viele Bins} $\rightarrow$ zackig, verrauscht
    \item Regeln:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Sturges}: konservativ, eher für normalverteilte Date
    \item \textbf{$\sqrt{n}$-Regel}: einfache Faustregel
    \item \textbf{Freedman–Diaconis (FD)}: robust, theoretisch fundiert, meist beste Wahl bei realen Daten
\end{itemize}
\end{itemize}
IMMER dokumentieren!

\subsection{Histogramme interpretieren}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Schiefe}: Rechts- oder linksschief erkennbar
    \item \textbf{Moden}: Anzahl der „Gipfel“ zeigt mögliche Mischungen/Subgruppen
    \item \textbf{Lücken}: können auf Clusterung oder fehlende Werte hinweisen
    \item \textbf{Tails}: zeigen Extremwerte oder Heavy Tails
\end{itemize}

\subsection{Histogramm vs. KDE}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Histogramm}
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Intuitiv, zählt echte Häufigkeite
    \item Stabil bei kleinen Stichproben
    \item Abhängig von Bin-Wahl
    \end{itemize}
    \item \textbf{KDE}
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Glatt, zeigt Form der Verteilung deutlicher (Schiefe, Multimodalität)
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item abhängig von Bandbreite
\end{itemize}
    \end{itemize}
\end{itemize}

\section{Box- und Violinplot}

\subsection{Vergleich Box vs. Violin}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Boxplot} - robust \& kompakt (Median + IQR)
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item \textbf{Stärken}: Kompakt, robust, standardisiert, Ausreißer klar sichtbar
        \item \textbf{Schwächen}: Zeigt keine Details der Verteilungsform
        \item Boxplot mit Rohdatenpunkten (Strip/Swarm)
    \end{itemize}
    \item \textbf{Violonplot} - formreich, zeigt Moden \& Tails
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item \textbf{Stärken}: Zeigt Dichteform \& Multimodalität, Quartile möglich
        \item \textbf{Schwächen}: Bandbreitenabhängig, schwerer zu lesen
        \item Best Practice: Violinplot mit Quartilen
    \end{itemize}
\end{itemize}

\subsection{Gruppenvergleiche}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Mehrere Gruppen nebeneinander vergleichen:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Boxplot $\rightarrow$ Fokus auf Median \& IQR-Vergleich
        \item Violinplot $\rightarrow$ Fokus auf Unterschiede in Form, Moden \& Tails
    \end{itemize}
    \item Interpretation:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Median A \> Median B $\rightarrow$ klare Lageunterschiede
        \item Überlappung der IQRs $\rightarrow$ keine klare Trennung
        \item Formunterschiede (Violin): Hinweis auf Mischungen oder Heterogenität in Gruppen
    \end{itemize}
\end{itemize}

\subsection{Zipfel des Violinplots}
Basiert auf KDE (es wird über die Datenpunkte eine glatte Kurve gelegt, die theoretisch nie ganz aufhören) $\Rightarrow$ Am unteren und oberen Rand der Violine entstehen manchmal kleine \textbf{„Zipfel“} (Bereiche, in denen die Kurve weiterläuft, obwohl dort keine echten Daten vorliegen) $\rightarrow$ \textbf{Rand-Effekt (boundary effect)}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Analytisch}: kein Problem – die Zipfel ändern nichts an der Aussage über Form, Schiefe oder Multimodalität
    \item \textbf{Visuell}: kann verwirrend wirken, weil es so aussieht, als gäbe es Werte außerhalb des tatsächlichen Datenbereichs
\end{itemize}
$\Rightarrow$ In Python/Seaborn (und auch in R): \verb|sns.violinplot(data=df, x="species", y="flipper_length_mm", cut=0)|
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \verb|cut = 0| = Kurve stoppt genau an den echten Daten $\rightarrow$ keine Zipfel mehr
    \item Die Zipfel verschwinden und die Violine endet genau dort, wo die Daten liegen
\end{itemize}

\subsection{Stripplot und Swarmplot}
\textbf{Stripplot} = Stellt jeden Datenpunkt als Punkt dar, optional mit leichtem Jitter (zufälliges horizontales Verschieben), um Überlagerungen zu vermeiden.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Vorteile}: Schnell, robust bei großem n
    \item \textbf{Nachteile}:Überdeckung möglich
\end{itemize}
\textbf{Swarmplot} = Ordnet die Punkte wie Bienenwaben an (kollisionsfrei). Dadurch wird die lokale Dichte sichtbar, keine Punkte überlappen.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Vorteile}: Lokale Dichte sichtbar, klar
    \item \textbf{Nachteile}: Langsamer, bei großem n überladen
\end{itemize}
\textbf{Best Practice}: 
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Für kleine bis mittlere Stichproben: Swarmplot (zeigt Details)
    \item Für große Stichproben: Stripplot mit Transparenz (zeigt Verteilung, ohne zu überladen)
\end{itemize}

\section{ECDF und QQ}

\subsection{ECDF (Empirical Cumulative Distribution Function)}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Empirische Verteilungsfunktion einer Stichprobe
    \item $\hat{F}(x) = \frac{1}{n} \sum 1\{x_i \leq x\}$
    \item Eigenschaften:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Monoton steigend von 0 bis 1
        \item Jedes Datenpunkt erzeugt einen Sprung
        \item Keine Bins nötig $\rightarrow$ \textbf{binfreie Darstellung}
    \end{itemize}
    \item Ermöglicht das direkte Ablesen von \textbf{Quantilen}:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Median = Punkt bei $F(x)=0.5$ 
        \item Q1 bei $F(x)=0.25$, Q3 bei $F(x)=0.75$ $\rightarrow$ direkt aus der Kurve ablesbar
    \end{itemize}
    \item Besonders hilfreich bei \textbf{kleinen Datensätzen} oder \textbf{schiefen Verteilungen}, weil Histogramme dort irreführend sein können
    \item Sehr geeignet für Gruppenvergleiche: mehrere ECDFs nebeneinander legen → Verschiebungen und Unterschiede sofort sichtbar
\end{itemize}
\textbf{ECDF: Zahl $\rightarrow$ Prozent, Quantil: Prozent $\rightarrow$ Zahl}

\subsection{QQ-Plot (Quantile-Quantile-Plot)}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Vergleicht die \textbf{Quantile einer Stichprobe} mit den \textbf{Quantilen einer Referenzverteilung}
    \item Aufgetragen:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item x-Achse = theoretische Quantile
        \item y-Achse = empirische Quantile
    \end{itemize}
    \item Liegen die Punkte auf einer Geraden, passt die Verteilung zur Referenz
\end{itemize}
\textbf{Interpretation}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Gerade Linie}: gute Übereinstimmung
    \item \textbf{Krümmung oben/unten}: Schiefe
    \item \textbf{Abweichungen in den Rändern}: Heavy Tails (dicke Ausläufer)
    \item \textbf{S-förmig}: dünnere Tails als Referenz (light tails)
\end{itemize}
\textbf{QQ-Plot = Diagnose-Tool für Schiefe \& Tails}

\subsection{Ausreißer-Analyse}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{ECDF} $\rightarrow$ Quantile präzise ablesen, Gruppen robust vergleichen
    \item \textbf{QQ-Plot} $\rightarrow$ prüfen, ob Daten annähernd normalverteilt sind (wichtig für klassische Tests) oder ob Heavy Tails/Ausreißer vorliegen
    \item Beides ergänzt Histogramm \& KDE:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Histogramm/KDE: visuell-intuitiv
        \item ECDF/QQ: mathematisch präziser und robuster
    \end{itemize}
\end{itemize}

\section{Korrelation \& Zusammenhang}

\subsubsection{Richtung und Stärke des Zusammenhangs}
Der \textbf{Korrelationskoeffizient $r$} beschreibt Richtung und Stärke eines linearen Zusammenhangs zwischen zwei metrischen Variablen.\\
$r \in [-1,\, 1]$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item $r > 0$ $\rightarrow$ positiver Zusammenhang:
hohe X-Werte gehen mit hohen Y-Werten einher
    \item $r < 0$ $\rightarrow$ negativer Zusammenhang:
hohe X-Werte gehen mit niedrigen Y-Werten einher
    \item $r = 0$ $\rightarrow$ kein linearer Zusammenhang
(aber es kann trotzdem ein nichtlinearer bestehen!)
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{img/Richtungen_des_Zusammenhangs.png}
\end{figure} 
Die Stärke wird durch den Betrag $|r|$ angegeben:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item $|r| \approx 0$: kein oder sehr schwacher linearer Zusammenhang
    \item $|r|$ nahe 1: starker Zusammenhang
\end{itemize}

\subsubsection{Interpretation des Korrelationskoeffizienten $r$}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{15pt} 
\begin{tabular}{llll}
\hline
\textbf{$r$-Wert} & \textbf{Richtung} & \textbf{Stärke} & \textbf{Interpretation (sprachlich)} \\
\hline
$-1.0$ bis $-0.9$ & negativ & sehr stark & Fast perfekter gegenläufiger Zusammenhang \\
$-0.9$ bis $-0.7$ & negativ & stark & Klarer Trend: je mehr $X$, desto weniger $Y$ \\
$-0.7$ bis $-0.4$ & negativ & mittel & Merklicher, aber nicht dominanter Zusammenhang \\
$-0.4$ bis $-0.2$ & negativ & schwach & Leichter gegenläufiger Trend, viel Rauschen \\
$-0.2$ bis $0.2$ & -- & keiner & Praktisch kein linearer Zusammenhang \\
$0.2$ bis $0.4$ & positiv & schwach & Leichter gemeinsamer Trend \\
$0.4$ bis $0.7$ & positiv & mittel & Stabiler, aber nicht perfekter Zusammenhang \\
$0.7$ bis $0.9$ & positiv & stark & Klarer Trend: $X$ und $Y$ steigen gemeinsam \\
$0.9$ bis $1.0$ & positiv & sehr stark & Fast perfekter Gleichlauf \\
\hline
\end{tabular}
\end{table}

\subsection{Korrelation vs. Kausalität}
\textbf{Korrelation beschreibt ein Muster, keine Ursache.}\\
$X \uparrow \Rightarrow Y \uparrow \rightarrow$ also „X verursacht Y“ $\rightarrow$ \textbf{nicht zwingend richtig!}\\ Der beobachtete Zusammenhang kann durch Zufall, eine Drittvariable oder umgekehrte Kausalrichtung entstehen.\\
$\rightarrow$ Eine Konfundierung (Drittvariable) liegt vor, wenn eine dritte Variable Z sowohl X als auch Y beeinflusst.\\
$\rightarrow$ Nie ohne kontrolliertes \textbf{Experiment} oder \textbf{theoretisches Modell} aus Korrelation auf Kausalität schließen!

\subsection{Kovarianz}
Die Kovarianz misst, wie stark zwei Variablen gemeinsam von ihren Mittelwerten abweichen.
$\rightarrow$ Sie zeigt, ob und wie $X$ und $Y$ „miteinander schwingen“: $\text{Cov}(X,Y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$\\
\noindent
\begin{minipage}{0.40\textwidth}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{img/Bildschirmfoto 2025-10-07 um 11.52.42.png}
\end{figure}
\end{minipage}\hfill
\begin{minipage}{0.56\textwidth}
   \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Quadrant I \& III: beide Variablen über/unter Mittelwert $\rightarrow$ positive Produkte $\rightarrow$ positive Kovarianz
    \item Quadrant II \& IV: eine über, die andere unter $\rightarrow$ negative Produkte $\rightarrow$ negative Kovarianz 
    \item Wenn sich die Abweichungen zufällig ausgleichen $\rightarrow$ Produkt $\approx 0$ $\rightarrow$ \textbf{kein linearer Zusammenhang}
\end{itemize}
\end{minipage}

\subsubsection{Grenzen der Kovarianz}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Einheitenanhängig}: \\
    $\rightarrow$  z. B. Euro × Jahre oder cm × kg → schwer vergleichbar
    \item \textbf{Größenabhängig}:\\
    $\rightarrow$ größere Wertebereiche $\Rightarrow$ automatisch größere Kovarianz\\
    $\rightarrow$ nicht normiert $\rightarrow$ keine Vergleichbarkeit zwischen Variablen oder Datensätzen
    \item \textbf{Lösung}: Normierung $\rightarrow$ \textbf{Pearson-Korrelation}
\end{itemize}

\subsection{Anscombe’s Quartett}
Das Anscombe’s Quartett (Francis John Anscombe, 1973) besteht aus einer Gruppe von vier Datensätzen, die grafisch sehr unterschiedlich aussehen, aber fast identische deskriptive Kennzahlen haben, z. B.:\\
\noindent
\begin{minipage}{0.35\textwidth}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Gleicher Mittelwert von $x$ und $y$
    \item Gleiche Varianz von $x$ und $y$
    \item Gleiche Kovarianz
    \item Gleicher Korrelationskoeffizient r
\end{itemize}
\end{minipage}\hfill
\begin{minipage}{0.61\textwidth}
    \begin{figure}[H]
       \includegraphics[width=1\linewidth]{img/Quartett2.png}
   \end{figure}
\end{minipage}
$\rightarrow$ In Zahlen sehen sie „gleich“ aus – aber \textbf{grafisch völlig unterschiedlich}! $\rightarrow$ Immer plotten!

\section{Kovarianz \& Pearson-Korrelation}

\subsection{Von der Kovarianz zur Korrelation}
Die Kovarianz ist nicht standardisiert und hängt von den Einheiten ab (z. B. CHF × Jahre).\\
$\rightarrow$ Sie ist daher nicht vergleichbar zwischen Variablen oder Datensätzen (PROBLEM: Skalenabhängigkeit)\\
\textbf{Lösung}: Standardisierung durch Division mit den Standardabweichungen von $X$ und $Y$.\\
$r = \frac{\text{Cov}(X,Y)}{s_X s_Y}$ $\Rightarrow$ Das ergibt die \textbf{Pearson-Korrelation $r$}

\subsection{Pearson-Korrelation}
\textbf{$r = \frac{\text{Cov}(X,Y)}{s_X s_Y}$}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item immer im Intervall $[-1, +1]$
    \item +1 = perfekter positiver linearer Zusammenhang
    \item –1 = perfekter negativer linearer Zusammenhang
    \item 0 = kein linearer Zusammenhang
\end{itemize}

\begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{img/PearsonVergleich.png}
\end{figure}

\subsubsection{Wichtige Eigenschaften von Pearson $r$}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Skalenunabhängig} $\rightarrow$ bleibt gleich, egal ob z. B. in CHF oder \$
    \item \textbf{Empfindlich gegenüber Ausreißern!} $\rightarrow$ Ein einziger Extremwert kann r massiv verändern
    \item Nur für lineare Zusammenhänge sinnvoll $\rightarrow$ Nicht-lineare, aber monotone Beziehungen werden unterschätzt.
    \item Voraussetzungen:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item metrische Variablen (Intervall-/Ratioskala)
        \item annähernde Normalverteilung
        \item keine starken Ausreißer
    \end{itemize}
\end{itemize}

\subsubsection{Anteil erklärter Varianz}
\noindent
\begin{minipage}{0.46\textwidth}
$r^2$ als Bestimmtheitsmaß verwendet, der Korrelationskoeﬃzient $r$ ist nicht der Erklärungsanteil!\\
$r^2 = \text{Anteil der Varianz von Y, der durch X erklärt wird.}$\\
Beispiel: $r = 0.8 \Rightarrow r^2 = 0.64 \rightarrow 64 \%$ der Unterschiede in $Y$ hängen mit $X$ zusammen.\\
\textbf{Aber: Das ist keine Kausalität!}
\end{minipage}\hfill
\begin{minipage}{0.50\textwidth}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{img/UnterschiedR.png}
\end{figure}
\end{minipage}

\section{Spearman \& Kendall – Rangkorrelationen}
Pearson-$r$ misst nur lineare Zusammenhänge und reagiert empfindlich auf Ausreißer oder Schiefe. Aber in der Realität sind viele Zusammenhänge nichtlinear, aber trotzdem systematisch $\Rightarrow$ Dafür brauchen wir rangbasierte Maße, die die Ordnung statt die exakten Werte vergleichen.\\
\\
\textbf{Spearman} ($\rho$) und \textbf{Kendall} ($\tau$) beruhen auf \textbf{Rängen statt Rohwerten}. $\rightarrow$ Extremwerte werden abgefedert und nur die relative Ordnung wird bewertet

\subsection{Spearman-Korrelation ($\rho$)}
Ersetzt die Werte durch \textbf{Ränge} und berechnet dann Pearson-$r$ auf diesen Rängen: $\rho = r(\text{rank}(X), \text{rank}(Y))$\\
Das ergibt ein Maß für den monotonen Zusammenhang (egal ob linear oder gekrümmt).
\\
Formel (für kleine Datensätze ohne „Ties“): \textbf{$\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}$} \\mit $d_i$ = Rangdifferenz der i-ten Beobachtung.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item $\rho = +1 \rightarrow$ perfekt gleichgerichtete Ränge
    \item $\rho = -1 \rightarrow$ perfekt umgekehrte Ränge
    \item $\rho = 0 \rightarrow$ kein monotoner Zusammenhang
\end{itemize}
\textbf{Eigenschaften:}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Misst Monotonie, nicht nur Linearität
    \item Robust gegen Ausreißer (weil Ränge stabil bleiben)
    \item Keine Normalverteilungs-Annahme nötig
    \item Gleiche Einheit wie Pearson ($[-1, +1]$)
\end{itemize}

\subsubsection{Gleiche Werte (Ties)}
Wenn Werte gleich sind $\rightarrow$ d\textbf{urchschnittlicher Rang}:
\begin{table}[h!]
\setlength{\tabcolsep}{12pt}   
\begin{tabular}{ccc}
\textbf{Wert} & \textbf{Rang (ohne Ties)} & \textbf{Rang (mit Ties)} \\ \hline
10 & 1 & 1 \\
20 & 2 & 2.5 \\
20 & 3 & 2.5 \\
40 & 4 & 4 \\
\end{tabular}
\end{table}\\
$\rightarrow$ Beide „20er“ bekommen Rang $(2+3)/2 = 2.5$. \\
\textbf{Das bewahrt die Ordnung und macht Spearman robust bei doppelten Werten!}

\subsection{Kendall’s Tau ($\tau$)}
Kendall vergleicht \textbf{Paarordnungen}:\\
Für jedes Paar ($x_i, y_i$) und ($x_j, y_j$) prüft man:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item konkordant: beide gleich geordnet (steigend/steigend oder fallend/fallend)
    \item diskordant: entgegengesetzt geordnet
\end{itemize}
$\tau = \frac{\text{(konkordante Paare)} - \text{(diskordante Paare)}}{\text{Gesamtzahl der Paare}}$

\textbf{Eigenschaften}:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Wertebereich $[-1, +1]$
    \item Etwas „gedämpfter“ als Spearman (kleinere Beträge)
    \item Besonders stabil bei vielen gleichen Rängen (Ties) oder kleinen Stichproben
    \item Misst dieselbe Idee wie Spearman, aber auf Paar-Ebene statt Rangdifferenzen
\end{itemize}

\subsection{Vergleich Spearman vs. Kendall}
\begin{table}[h!]
\setlength{\tabcolsep}{12pt}     
\begin{tabular}{l c c}
\textbf{Merkmal} & \textbf{Spearman ($\rho$)} & \textbf{Kendall ($\tau$)} \\ \hline
Idee & Pearson auf Rängen & Vergleich von Paaren \\
Misst & Monotone Ordnung & Konkordanz/Diskordanz \\
Robustheit & robust & sehr robust \\
Sensitivität & höher (größere Werte) & gedämpft \\
Eignung & große $n$, metrisch & kleine $n$, viele Ties \\
Bereich & [$-1$, $+1$] & [$-1$, $+1$] \\
\end{tabular}
\end{table}

\section{Wann welche Korrelation?}
\noindent
\begin{minipage}{0.62\textwidth}
\begin{table}[H]
\setlength{\tabcolsep}{12pt}
\begin{tabular}{l c}
\textbf{Situation} & \textbf{Empfehlung} \\ \hline
Lineare, metrische Beziehung & Pearson $r$ \\
Monoton, aber nicht linear & Spearman $\rho$ \\
Viele gleiche Werte oder kleine Stichprobe & Kendall $\tau$ \\
Ausreißer vorhanden & Rangmaß ($\rho$ oder $\tau$) \\
Ordinale Skalen (z.\,B.\ Zufriedenheit 1--5) & Rangmaß \\
\end{tabular}
\end{table}
\end{minipage}\hfill
\begin{minipage}{0.34\textwidth}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Pearson}: misst \textbf{lineare Stärke}
    \item \textbf{Spearman}: misst \textbf{monotone Richtung}
    \item \textbf{Kendall}: misst \textbf{Rangordnung}
\end{itemize}
\end{minipage}

\section{Visualisierung von Korrelationen}

\subsection{Regressionslinie im Scatterplot}
\noindent
\begin{minipage}{0.48\textwidth}
Eine Regressionslinie (Trendlinie) verdeutlicht den Zusammenhang – \textbf{ohne Kausalität zu implizieren!}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Steigung}: Richtung des Zusammenhangs
    \item \textbf{Schatten (Konfidenzintervall)}: Unsicherheit
    \item \textbf{Nichtlinearität}: erkennbar, wenn Punkte deutlich von der Linie abweichen
\end{itemize}
Wenn die Linie gekrümmt oder unpassend erscheint $\rightarrow$ Pearson ist ungeeignet $\rightarrow$  Spearman oder Kendall nutzen
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
   \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{img/Bildschirmfoto 2025-10-11 um 11.53.09.png}
    \end{figure}
\end{minipage}

\subsection{Best Practice}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Achsen immer beschriften + Einheiten angeben
    \item Farben sinnvoll nutzen 
    \item Einheitliche Skalen bei Gruppenplots
    \item Transparenz (alpha) gegen Overplotting
    \item Trends immer kritisch interpretieren – \textbf{Trend $\neq$ Ursache!}
\end{itemize}

\section{Simpson-Paradox}
Korrelation ist ein mächtiges Werkzeug, aber in der Praxis häufig falsch interpretiert. Viele scheinbar „offensichtliche“ Trends verschwinden oder kehren sich sogar um, wenn man Drittvariablen oder Gruppen berücksichtigt.$\rightarrow$ Simpson-Paradox\\
\noindent
\begin{minipage}{0.48\textwidth}
\textbf{Definition}: Das Simpson-Paradox beschreibt den Effekt, dass ein Zusammenhang zwischen zwei Variablen in aggregierten Daten anders oder sogar entgegengesetzt ist als in den einzelnen Gruppen.\\
$\rightarrow$ Konfundierung: Eine Drittvariable (Z) verzerrt den scheinbaren Zusammenhang zwischen X und Y. \\
Ohne Kontrolle von Z $\Rightarrow$ Scheinkorrelation
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/SimpsonParadox.png}
\end{figure}
\end{minipage}
$~~$\\
\textbf{Praxisfallen}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Korrelation $\neq$ Kausalität
    \item Ausreißer \& Nichtlinearität
    \item Heterogene Gruppen (Aggregationseffekte)
\end{itemize}

\subsection{Umgang mit Simpson-Eﬀekt}
Stratifizierte Analysen vermeiden Fehlschlüsse. Immer auf versteckte Variablen prüfen!
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Stratifizierung: Daten nach Konfundierer aufsplitten und separate Analysen pro Gruppe durchführen\\
    $\rightarrow$ Vergleich Gesamt vs. innerhalb: Wenn sie widersprüchlich sind $\rightarrow$ Gefahr!
    \item Partielle Korrelation (lineares Herausrechnen des Z-Einflusses)
    \item Regressionsmodelle mit Kontrollvariablen
\end{itemize}

\subsection{Partielle Korrelation}
Um echte Zusammenhänge zu erkennen, musst du Drittvariablen (Z) herausrechnen $\rightarrow$ Die partielle Korrelation \textbf{misst} dann \textbf{den Zusammenhang zwischen zwei Variablen (X und Y)}\\
$r_{xy \cdot z} = \dfrac{r_{xy} - r_{xz}r_{yz}}{\sqrt{(1 - r_{xz}^2)(1 - r_{yz}^2)}}$\\
$r_{XY}$: einfache (bivariate) Korrelation zwischen X und Y\\
$r_{XZ}$, $r_{YZ}$: Korrelationen von X und Y mit Z\\
\textbf{Interpretation}:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Wenn $r_{XY\cdot Z} \approx r_{XY}$: Kaum Einfluss von Z
    \item Wenn $r_{XY\cdot Z} \approx 0$, obwohl $r_{XY}$ hoch war: der scheinbare Zusammenhang X–Y war vollständig durch Z erklärt (Konfundierung!)
    \item $r_{XY\cdot Z} \neq r_{XY}$: zeigt, wie stark Z den ursprünglichen Zusammenhang verzerrt hat
\end{itemize}

\section{Wahrscheinlichkeit und Verteilungen}

\subsection{Venn-DIagramme}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{
    img/Venn.png}
\end{figure}

\subsection{Die drei Axiome nach Kolmogorov}
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Nichtnegativität}: $Pr(A) \geq 0$
    \item \textbf{Normierung}: $Pr( \Omega ) = 1$
    \item \textbf{Additivität}: Falls $A \cap B = \varnothing \Rightarrow Pr(A \cup B) = Pr(A)+Pr(B)$
\end{enumerate}

\subsection{Das Gesetz vom komplementären Ereignis}
Jedes Ereignis hat ein Gegenteil: zusammen ergeben sie Sicherheit. Komplement: $A^c =$ "A tritt nicht ein"\\
$Pr(A^c) = 1 - Pr(A)$

\subsection{Das Gesetz der Vereinigung}
Berücksichtigt Überschneidungen: $Pr(A \cup B) = Pr(A) + Pr(B) - Pr(A \cap B)$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item „Oder“ ist \textbf{inklusiv}
    \item Wenn A und B unabhängig sind: $Pr(A \cap B) = Pr(A) \times Pr(B)$
\end{itemize}

\subsection{Unabhängigkeit}
Das eine Ereignis verändert nicht die Wahrscheinlichkeit des anderen.\\
$Pr(A \cap B) = Pr(A) \times Pr(B)$\\
Im Venn-Diagramm: Überlappung vorhanden, aber \textbf{Flächenverhältnis bleibt konstant}.\\
Unabhängigkeit $\neq$ keine Überlappung!

\subsection{Bedingte Wahrscheinlichkeit}
Wenn zusätzliche Information vorhanden ist, verändert sich die Wahrscheinlichkeit.\\
$Pr(A | B) = \frac{Pr(A \cap B)}{Pr(B)}$\\
$Pr(A | B) =$ Wahrscheinlichkeit von A, unter der Bedingung, dass B eingetreten ist\\
$Pr(A | B) \neq Pr(B | A)$!

\subsubsection{Das Theorem von Bayes}
\noindent
\begin{minipage}{0.52\textwidth}
Bayes zeigt, \textbf{wie Vorwissen (Prior)} mit \textbf{neuer Evidenz (Daten)} kombiniert wird.\\
$Pr(B | A) = \frac{Pr(A | B) Pr(B)}{Pr(A)}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Prior}: Vorwissen über B
    \item \textbf{Likelihood}: Wie plausibel sind Daten unter B
    \item \textbf{Evidence}: Gesamtwahrscheinlichkeit der Beobachtung
    \item \textbf{Posterior}: Aktualisierte Überzeugung nach Daten
\end{itemize}
\end{minipage}\hfill
\begin{minipage}{0.44\textwidth}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/BayesMalung.png}
\end{figure}
\end{minipage}
\textbf{Bayesian Thinking}: Wissen = dynamisch → Posterior = Prior × Evidenz

\subsection{Risikomasse}
\textbf{Warum?} Statistik untersucht nicht nur Wahrscheinlichkeiten, sondern auch Unterschiede zwischen Wahrscheinlichkeiten – also, ob ein bestimmter Faktor (E) das Risiko eines Ereignisses (D) verändert.

\subsubsection{Risikodifferenz}
Zeigt den absoluten Zusatznutzen oder -schaden durch den Faktor E.\\
$ER = Pr(D|E) - Pr(D|E^c) \rightarrow$ Misst den absoluten Unterschied in Prozentpunkten

\subsubsection{Relatives Risiko (RR) \& Odds Ratio (OR)}
Diese Maße zeigen den \textbf{Faktor}, um den ein Risiko \textbf{steigt oder sinkt}.\\
\textbf{Relatives Risiko}: $RR = \frac{Pr(D|E)}{Pr(D|E^c)}$\\
Gibt an, wie viel-fach höher (oder niedriger) das Risiko mit E ist.\\
\textbf{Odds Ratio}: $OR = \frac{Pr(D|E)/(1-Pr(D|E))}{Pr(D|E^c)/(1-Pr(D|E^c))}$\\
Misst das Verhältnis der Quoten (odds), also der Chancen für $D$ relativ zu $D^c$. $\rightarrow$ Wird in der logistischen Regression verwendet (Machine-Learning-Bezug)\\
Bei \textbf{seltenen Ereignissen} gilt näherungsweise: $OR \approx RR$.

\subsection{Zufallsvariablen \& Verteilungen}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Binomial}: Zählprozesse, n Versuche, p = Erfolgs-Wahrscheinlichkeit\\
    \item $Pr(X = k) = \binom{n}{k}p^k(1-p)^{n-k}$
    \item \textbf{Poisson}: Seltene Ereignisse im Intervall\\
    \item $Pr(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}$
    \item \textbf{Normal}: Summeneffekte / Messfehler $\rightarrow$ Symmetrisch
    \item \textbf{Exponential}: Zeit bis zum nächsten Ereignis
    \item \textbf{Uniform}: vollständige Unkenntnis – alle gleich wahrscheinlich
\end{itemize}

\subsection{Erwartungswert}
Der Erwartungswert $E(X)$ ist der Schwerpunkt einer Verteilung – also das mathematische Mittel aller möglichen Ausgänge.\\
Er beschreibt den langfristigen Durchschnitt, nicht das Ergebnis eines einzelnen Experiments.\\
Diskret: $E(X)=\sum x_i \,Pr(X=x_i)$\\
Kontinuierlich: $E(X)=\int x\,f(x)\,dx$

\subsection{Varianz und Standardabweichung}
Die Varianz misst, wie weit Zufallswerte vom Erwartungswert entfernt liegen $\rightarrow$ Streuung oder Unsicherheit der Verteilung\\
$Var(X)=E[(X-E(X))^2], \quad SD=\sqrt{Var(X)}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Kleine Varianz $\rightarrow$ Werte liegen eng um den Erwartungswert
    \item Große Varianz $\rightarrow$ starke Streuung, unregelmäßiger Zufall
\end{itemize}

\subsection{Gesetz der grossen Zahlen (LLN)}
\textbf{Viele Zufälle ergeben Regelmäßigkeit.}\\
Wenn $X_1,\dots,X_n$ unabhängig und identisch verteilt sind, gilt:
$\bar{X}n=\frac{1}{n}\sum{i=1}^n X_i \xrightarrow[n\to\infty]{} E[X]$\\
Der \textbf{Stichprobenmittelwert konvergiert} gegen den Erwartungswert.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/GrosseZahlen.png}
\end{figure}

$\Rightarrow$ \textbf{Zufall wird vorhersagbar}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Einzelbeobachtung = Rauschen; viele Beobachtungen = Signal
    \item Aggregierte Werte (Mittel, Anteile) $\rightarrow$ verlässlichere Information
    \item In Data Science: Durchschnitt = Signal über Rauschen
    \item LLN bedeutet: „Rauschen löscht sich im Schnitt aus.“
\end{itemize}

\section{Schätzen $\&$ Konfidenzintervalle}

\subsection{Bootstrap – empirische Unsicherheitsschätzung}
Konfidenzintervalle und Standardfehler basierten auf theoretischen Verteilungen (z. B. Normal- oder t-Verteilung).
Problem: Diese Annahmen sind oft unrealistisch – insbesondere bei: kleinen Stichproben, unbekannter oder schiefer Verteilung, nichtparametrischen Verfahren.\\
Bootstrap bietet eine \textbf{verteilungsfreie}, \textbf{empirische Methode}, um die Unsicherheit (z. B. Standardfehler, CI) zu schätzen.
$\rightarrow$ simuliert wiederholte Stichproben aus der vorhandenen Stichprobe selbst\\
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/BootstrapVergleich.png}
\end{figure}

\subsection{Margin of Error}
Wenn eine Stichprobe verwendet wird, um einen Parameter der Population, wie den Anteil oder den Mittelwert, zu schätzen, ist diese Schätzung immer mit Unsicherheiten verbunden. Der Margin of Error quantifiziert diese Unsicherheit, indem er einen Bereich um die Schätzung angibt, der wahrscheinlich den tatsächlichen Wert enthält.\\
Der Margin of Error hängt von mehreren Faktoren ab:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Stichprobengröße ($n$)}: Je größer die Stichprobe, desto kleiner der MoE, weil die Schätzung genauer wird.
    \item \textbf{Stichprobenfehler ($\sigma$ oder $p$)}: Die Streuung der Daten beeinflusst den MoE.
    \item \textbf{Konfidenzniveau ($1-\alpha$)}: Das ist die Wahrscheinlichkeit, mit der der wahre Wert im angegebenen Bereich liegt. Typische Werte sind 90\%, 95\% oder 99\%.
\end{itemize}
Die allgemeine Formel für den Margin of Error bei einem Mittelwert lautet:\\
$MoE = z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}$\\
Hierbei ist:\\
$\rightarrow$ $z_{\alpha/2}$ der kritische Wert aus der Standardnormalverteilung, der vom gewünschten Konfidenzniveau abhängt (z.\,B. ca. 1{,}96 für 95\%).\\
$\rightarrow$ $\sigma$ die Standardabweichung der Population (bei unbekannter $\sigma$ wird die Stichprobenstandardabweichung verwendet).\\
Bei Anteils-Schätzungen (z.\,B. Prozentsätzen) lautet die Formel:\\
$MoE = z_{\alpha/2} \times \sqrt{\frac{p(1-p)}{n}}$,
wobei $p$ der geschätzte Anteil ist.

\subsection{Standardabweichung und Standardfehler}
\textbf{SD}: Wie stark streuen die Daten innerhalb einer Stichprobe?\\
$\rightarrow$ Datenebene\\
$\rightarrow$ beschreibt die Variabilität in deiner Stichprobe\\
\textbf{SE}: Wie stark streut ein Schätzer (z. B. der Mittelwert) zwischen verschiedenen Stichproben?\\
$\rightarrow$ Schätzer\\
$\rightarrow$ beschreibt die Unsicherheit deiner Schätzung, also die Streuung von $\bar{X}$ oder $\hat{p}$ über viele gedachte Stichproben.\\
Für den Mittelwert: $E(\bar X) = \frac{\sigma}{\sqrt{n}}
\quad
\text{(in der Praxis: } \widehat{SE}(\bar X) = \frac{s}{\sqrt{n}} \text{)}$\\
Für einen Anteil:$SE(\hat p) = \sqrt{\frac{p(1-p)}{n}}
\quad
\text{(in der Praxis: } \widehat{SE}(\hat p) = \sqrt{\frac{\hat p (1-\hat p)}{n}} \text{)}$

\subsubsection{Quantile}
Quantile geben an, wie weit wir vom Zentrum der Verteilung weggehen müssen, um z. B. 95 $\%$ abzudecken.\\
Konfidenzintervall (KI) = Schätzer $\pm$ (Quantil · SE).\\
$\alpha$ = Irrtumswahrscheinlichkeit (z.B. $\alpha$ = 0.05 für 95 $\%$-KI)\\
KI-Idee (für µ) mit z:
$\bar{x} \pm z_{1-\alpha/2}\cdot SE(\bar{X}) \quad\Rightarrow\quad \bar{x} \pm 1.96\cdot SE(\bar{X})\ \text{bei 95 \%}$

\subsection{Typische (Stichproben) Bias-Arten}
Anforderungen an die Stichprobe:
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Zufällig (Jedes Element der Population hat die gleiche Chance, ausgewählt zu werden)$\rightarrow$ vermeidet systematische Verzerrungen
    \item Unabhängig (Der Wert einer Beobachtung beeinflusst nicht den Wert einer anderen) $\rightarrow$ Formeln für Varianz/SE stimmen
    \item Repräsentativ (Die Stichprobe spiegelt die Vielfalt und Struktur der Population wider) $\rightarrow$ Schätzung ist wirklich übertragbar auf die Population
\end{enumerate}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item textbf{Selektionsbias}: Nur bestimmte Personen werden befragt
    \item textbf{Nonresponse-Bias}: Wer nicht antwortet, unterscheidet sich vom Rest
    \item textbf{Abhängigkeiten}: Daten stammen aus Clustern oder Zeitreihen
\end{itemize}

\subsection{z-Verteilung}
= Standardnormalverteilung mit $\mu = 0,\quad \sigma = 1$\\
Wir standardisieren: $z = \frac{x - \mu}{\sigma}$ $\rightarrow$ z gibt an, wie viele Standardabweichungen ein Wert vom Mittelwert entfernt ist.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item symmetrisch, glockenförmig
    \item ca. 68 $\%$ der Werte zwischen -1 und +1
    \item ca. 95 $\%$ der Werte zwischen -1.96 und +1.96
\end{itemize}
In Konfidenzintervallen verwenden wir z-Quantile, wenn
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item die Verteilung (nahezu) normal ist und
    \item die Populationsstreuung $\mu$ bekannt ist oder n so gross, dass $s \approx \sigma \; (\text{CLT})$
\end{itemize}

\subsection{t-Verteilung}
n der Praxis kennen wir $\mu$ meistens nicht, sondern schätzen sie aus der Stichprobe durch s. $\rightarrow$ Mehr Unsicherheit, insbesondere bei kleinem n.\\
t-Statistik: $t = \frac{\bar{x} - \mu}{s / \sqrt{n}}$ $\rightarrow$ Wie z, aber mit s statt $\sigma$ $\rightarrow$ zusätzliche Unsicherheit $\rightarrow$ dickere Tails.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item breitere Flanken (mehr Masse in den Rändern) als z-Verteilung
    \item hängt von den Freiheitsgraden $df = n-1$ ab:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item kleine df $\rightarrow$ sehr breit, unsicher
    \item grosse df $\rightarrow$ nähert sich der z-Verteilung an
    \end{itemize}
\end{itemize}
Für Konfidenzintervalle bei kleiner Stichprobe: $\bar{x} \pm t_{df,\,1-\alpha/2} \cdot \frac{s}{\sqrt{n}}$

\subsection{Punktschätzung}
Idee: Schätzen liefert Zahlen, mit denen man Entscheidungen treffen kann.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/WegVonDaten.png}
\end{figure}

\subsubsection{Bias $\&$ Varianz}
\textbf{Bias}: systematischer Fehler\\
\textbf{Varianz}: Zufallsschwankung\\
Beim Schätzen: Jede Stichprobe ist anders $\rightarrow$ jeder Schätzwert etwas anders $\Rightarrow$ \textbf{Varianz des Schätzers}\\
$\rightarrow$ Warum sind Schätzungen zufällig?\\
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Zufällige Stichprobe: wer/was in die Stichprobe kommt
    \item Modellunsicherheit: das angenommene Modell (z. B. Normalverteilung, Linearität) passt nie perfekt
\end{enumerate}
$\Rightarrow$ Schätzer haben keine fixe Zahl, sondern eine Verteilung:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Mittelwert dieser Verteilung $\rightarrow$ Bias (Abstand zur Wahrheit)
    \item Streuung dieser Verteilung $\rightarrow$ Varianz / SE
\end{itemize}
$\Rightarrow$ Mehr Daten $\rightarrow$ Varianz kleiner $\rightarrow$ stabilere Schätzungen
\\ \\
\textbf{Modell-Bias} entsteht, wenn wir bewusst ein vereinfachtes Modell verwenden
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item „Modell“ = Annahmen über:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Verteilungsform
        \item funktionale Beziehung
        \item konstante Varianz, Unabhängigkeit usw.
    \end{itemize}    
    \item Diese Annahmen sind nie perfekt wahr $\rightarrow$ leichter systematischer Fehler (Modell-Bias)
\end{itemize}
$\rightarrow$ Vereinfachte Modelle sind stabiler, interpretierbar und oft ausreichend genau für Entscheidungen

\subsubsection{Gütekriterien $\&$ MSE}
Gute Schätzer balancieren:
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item kleinen Bias (im Mittel nicht weit daneben)
    \item kleine Varianz (Stichproben-Schätzungen schwanken wenig)
\end{enumerate}
Beides zusammen fasst der Mean Squared Error (MSE): $\mathrm{MSE}(T) = \mathbb{E}\big[(T - \theta)^2\big] = \mathrm{Bias}(T)^2 + \mathrm{Var}(T)$\\
Ziel: \textbf{MSE minimieren} $\rightarrow$ guter Kompromiss zwischen:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item „nicht schief“ (Bias klein)
    \item „nicht zitterig“ (Varianz klein)
\end{itemize}
$\rightarrow$ Ein ganz unverzerrter Schätzer ist nicht unbedingt am besten, wenn seine Varianz sehr gross ist\\
$\rightarrow$ Leicht verzerrter Schätzer mit viel kleinerer Varianz kann einen kleineren MSE haben → insgesamt „besser“

\section{Hypothesentests}

\subsection{Grundidee eines Hypothesentests}
\begin{enumerate}
  \item Formuliere $H_0$ (Nullhypothese):
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
      \item „Kein Effekt“, „kein Unterschied“, „Parameter = Referenzwert“.
      \item Beispiele:
        \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
          \item Medikament hat keinen Effekt: $\mu_{\text{Med}} = \mu_{\text{Placebo}}$
          \item Kein Gender-Pay-Gap: $\mu_{\text{Frau}} = \mu_{\text{Mann}}$
          \item Anteil $= 0{,}5$: $p = 0{,}5$
        \end{itemize}
    \end{itemize}
  \item Formuliere $H_1$ (Alternativhypothese):
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
      \item „Es gibt einen Effekt / Unterschied“.
      \item Beispiele: $\mu_{\text{Med}} \neq \mu_{\text{Placebo}}, \ \mu_{\text{Frau}} \neq \mu_{\text{Mann}}, \ p \neq 0{,}5$
      \item Einseitig ($>$, $<$) oder zweiseitig ($\neq$) – je nach Fragestellung.
    \end{itemize}
  \item Wähle eine Teststatistik $T$.
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
      \item Eine Zahl, die „Abweichung von $H_0$“ misst.
      \item Typisch: $z$- oder $t$-Statistik, Differenz der Mittelwerte, Anteilschätzer usw.
    \end{itemize}
  \item Bestimme die Verteilung von $T$ unter $H_0$.
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
      \item z.\,B. Standardnormalverteilung oder $t$-Verteilung mit $\mathrm{df} = n-1$.
    \end{itemize}
  \item Berechne den beobachteten Wert $t_{\text{obs}}$ aus den Daten.
  \item Vergleiche $t_{\text{obs}}$ mit der Nullverteilung (p-Wert).
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
      \item p-Wert $=$ Wahrscheinlichkeit (unter $H_0$), einen Wert mindestens so extrem wie $t_{\text{obs}}$ zu erhalten.
      \item kleiner p-Wert $\Rightarrow$ starke Evidenz gegen $H_0$.
    \end{itemize}
  \item Entscheidungsregel:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
      \item Wenn $p \le \alpha \Rightarrow H_0$ verwerfen („statistisch signifikant“).
      \item Wenn $p > \alpha \Rightarrow H_0$ nicht verwerfen (Daten reichen nicht, um $H_0$ zu kippen).
    \end{itemize}
\end{enumerate}

\subsection{Fehler 1. und 2. Art $\&$ Teststärke (Power)}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} lcc}
\hline
Realität $\backslash$ Entscheidung & $H_0$ \emph{nicht} verwerfen & $H_0$ verwerfen \\
\hline
$H_0$ ist wahr   & korrekt                  & Fehler 1.\ Art ($\alpha$)           \\
$H_0$ ist falsch & Fehler 2.\ Art ($\beta$) & korrekt (Power $= 1-\beta$) \\
\hline
\end{tabular*}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Fehler 1. Art ($\alpha$)
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item $H_0$ ist wahr, aber wir verwerfen sie.
        \item False Positive“, Fehlalarm.
        \item Signifikanzniveau $\alpha$ ist die erlaubte Wahrscheinlichkeit für diesen Fehler (z. B. 5 $\%$).
        \item Wird vom Forschenden festgelegt
    \end{itemize}
    \item Fehler 2. Art ($\beta$)
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item $H_0$ ist falsch, aber wir verwerfen sie nicht.
        \item „False Negative“, übersehener Effekt
    \end{itemize}
    \item Teststärke / Power $= 1-\beta$
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Wahrscheinlichkeit, $H_0$ korrekt zu verwerfen, wenn sie tatsächlich falsch ist (echter Effekt)
        \item Ziel: $\alpha$ klein und Power gross
    \end{itemize}
\end{itemize}

\subsubsection{Einfluss auf Power}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item grösseres n $\rightarrow$ kleinere SE $\rightarrow$ höhere Power
    \item grösserer wahrer Effekt $\Delta$ $\rightarrow$ leichter zu entdecken $\rightarrow$ höhere Power
    \item kleineres $\alpha$ $\rightarrow$ strengere Schwelle $\rightarrow$ tendenziell niedrigere Power (Trade-off)
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/EinflussAufPower.png}
\end{figure}
\begin{center}
Größere Stichprobe $\rightarrow$ schmalere Kurve $\rightarrow$ geringere Überlappung $\rightarrow$ höhere Power
\end{center}

\subsection{p-Wert}
= Wahrscheinlichkeit, unter $H_0$ ein Ergebnis so extrem oder extremer zu beobachten wie das tatsächlich beobachtete.\\
$\rightarrow$ misst die Kompatibilität der Daten mit $H_0$. $\rightarrow$ Je kleiner $p$, desto schlechter passen die Daten zu einer Welt, in der $H_0$ stimmt.\\
$p = P\bigl(T \ge t_{\text{beobachtet}} \mid H_0\bigr)$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item $T$ = Teststatistik (z-, t-Wert, \dots)
  \item $t_{\text{beobachtet}}$ = Wert, den du aus deinen Daten ausrechnest
  \item Die Wahrscheinlichkeit wird \textbf{unter der Annahme $H_0$} berechnet.
\end{itemize}
Interpretation:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item \textbf{kleiner p-Wert} $\Rightarrow$ die beobachteten Daten wären unter $H_0$ \textbf{ungewöhnlich} \\
        $\Rightarrow H_0$ ist schlecht mit den Daten kompatibel $\Rightarrow$ spricht \textbf{gegen $H_0$}
  \item \textbf{grosser p-Wert} $\Rightarrow$ die Daten sind unter $H_0$ \textbf{typisch} \\
        $\Rightarrow$ nichts in den Daten spricht stark gegen $H_0$
\end{itemize}

\subsubsection{Entscheidungsregel (Zusammenhang zu $\alpha$)}
\begin{itemize}
  \item Du wählst ein \textbf{Signifikanzniveau} $\alpha$ 
  \item Entscheidungsregel:
  \begin{itemize}
    \item Wenn \textbf{$p \le \alpha$} $\Rightarrow$ Ergebnis „statistisch signifikant“ $\Rightarrow$ \textbf{$H_0$ verwerfen}
    \item Wenn \textbf{$p > \alpha$} $\Rightarrow$ \textbf{$H_0$ nicht verwerfen} (Daten reichen nicht, um $H_0$ zu kippen)
  \end{itemize}
\end{itemize}
\textbf{Wichtig:} „$H_0$ nicht verwerfen“ heisst \emph{nicht}, dass $H_0$ bewiesen ist – nur, dass die Daten mit $H_0$ \textbf{nicht im Widerspruch} stehen.

\subsection{t-Tests}
t-Tests sind immer Tests über Mittelwerte und benutzen die t-Verteilung, weil $\sigma$ unbekannt ist und durch s geschätzt wird.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{1-Sample}: Mittelwert einer Stichprobe vs. Referenzwert
    \item \textbf{2-Sample}: Mittelwerte zweier unabhängiger Gruppen vergleichen
    \item \textbf{Paired}: Vorher–Nachher / gepaarte Messungen derselben Personen
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/tTests.png}
\end{figure}

\subsubsection{Zwei-Stichproben-t-Test}
Prüft, ob sich zwei unabhängige Gruppen im Mittelwert unterscheiden.
\[
t = \frac{\bar{x}_1 - \bar{x}_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
\]
\[
s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
\]


\subsubsection{Gepaarter t-Test}
$\rightarrow$ Für Vorher–Nachher-Daten oder gepaarte Beobachtungen
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item Für jede Person $i$ eine Differenz bilden: $d_i = x_{i,\text{nachher}} - x_{i,\text{vorher}}$

  \item Mittelwert und SD der Differenzen:
  \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item $\bar{d}$ = Durchschnitt der $d_i$
    \item $s_d$ = SD der $d_i$
  \end{itemize}

  \item Hypothesen:
  \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item H$_0$: $\mu_d = 0$ (kein mittlerer Unterschied)
    \item H$_1$: $\mu_d \neq 0$ (oder $> 0$, $< 0$ einseitig)
  \end{itemize}

  \item Teststatistik: $t = \frac{\bar{d}}{s_d / \sqrt{n}}$
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item $n$ = Anzahl Paare / Personen
        \item $\mathrm{df} = n - 1$
    \end{itemize}
\end{enumerate}

\subsection{p-Hacking}
Wiederholtes Testen oder Subgruppen-Analysen, bis $p < 0.05$ erreicht ist\\
\textbf{Problem}: erhöht massiv die Wahrscheinlichkeit für Fehlalarme ($\alpha$-Inflation)

\subsection{A/B-Test}
= Hypothesentest, der zwei Varianten (A und B) miteinander vergleicht.

\subsubsection{Ablauf eines A/B-Tests}
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Randomisierung}: Personen werden zufällig A oder B zugeordnet.
    \item \textbf{Messen}: Wir erfassen eine Metrik (z. B. Conversion).
    \item \textbf{Testen}: Vergleich der Gruppen (t-Test oder $X^2$ bei Anteilen).
    \item \textbf{Effektgrösse}: Wie stark ist der Unterschied praktisch?
\end{enumerate}

\subsection{$X^2$-Test}
Prüft, ob beobachtete Häufigkeiten in einer Stichprobe signifikant von den erwarteten Häufigkeiten abweicht.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item Wir haben eine \textbf{Kontingenztafel} (z.\,B. A/B $\times$ Ja/Nein).
  \item Unter H$_0$ gilt: „Kein Unterschied / kein Zusammenhang“ \\
        $\Rightarrow$ z.\,B. gleiche Ja-Rate in A und B.
\end{itemize}
Wir berechnen: $\chi^2 = \sum_{i,j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item $O$ = observed (beobachtet)
  \item $E$ = expected (erwartet, wenn H$_0$ stimmt)
\end{itemize}
\textbf{Interpretation:}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item Kleine Abweichung O–E $\Rightarrow$ kleiner $\chi^2$-Wert
        $\Rightarrow$ $H_0$ gut vereinbar mit Daten.
  \item Grosse Abweichung O–E $\Rightarrow$ grosser $\chi^2$-Wert
        $\Rightarrow$ H$_0$ unwahrscheinlich $\Rightarrow p$ klein.
\end{itemize}

\subsubsection{Freiheitsgrade im $X^2$-Test}
Für eine $r \times c$-Tabelle ($r$ Zeilen, $c$ Spalten):$ df = (r-1)(c-1)$\\
In der $2 \times 2$-Tabelle (klassischer A/B-Fall): $r = 2, \ c = 2 \Rightarrow df = (2-1)(2-1) = 1.$

\subsection{Permutationstest}
\textbf{Frage}: Wir haben zwei Gruppen A und B und sehen eine Differenz (z. B. Mittelwert B – Mittelwert A).
Ist diese Differenz besonders, oder könnte sie leicht durch Zufall entstehen?\\
Unter $H_0$: „A und B sind gleich“ $\rightarrow$ Die Labels A/B sind austauschbar.\\
$\rightarrow$ Wenn $H_0$ stimmt, ist es egal, welche Personen als „A“ oder „B“ gelabelt sind.\\
$\rightarrow$ Jede zufällige Neuverteilung der Labels wäre genauso plausibel wie die Original-Zuordnung.\\
\textbf{Idee des Permutationstests}:
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Wir mischen die Gruppenlabels immer wieder zufällig durch.
    \item Für jede Durchmischung berechnen wir die neue Differenz $\Delta$ (z.B. Mittelwert(B) - Mittelwert(A)).
    \item Wir schauen, wie oft $\lvert \Delta^* \rvert$ mindestens so groß ist wie die beobachtete $\lvert \Delta \rvert$
\end{enumerate}
$\rightarrow$ So simulieren wir die Nullverteilung der Differenz, ganz ohne Normalverteilungsannahme.

\subsubsection{Schrittfolge}
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Beobachtete Differenz $\Delta$ berechnen
    \item Labels mischen: Daten werden zufällig auf Gruppen verteilt.
    \item Neue Differenz $\Delta^*$ berechnen $\rightarrow$ Schritte 2 und 3 werden sehr häufig wiederholt
    \item p-Wert bestimmen: Anteil der $\Delta^*$ mit $\lvert \Delta^* \rvert \ge \lvert \Delta \rvert$
\end{enumerate}

\subsection{Effektgrößen}
\textbf{Signifikanz $\neq$ Relevanz}\\
p-Werte sagen nur: «Unterschied auﬀällig?», nicht: «Unterschied wichtig?»\\
\textbf{Problem}: Bei grossen Stichproben wird fast alles „signifikant“ – auch winzige Effekte.\\
$\rightarrow$ Eﬀektgrössen messen Stärke eines Eﬀekts, unabhängig von n.

\subsubsection{Cohen’s d}
\textbf{Cohen's d} misst die Distanz zweier Mittelwerte \textbf{in Einheiten der gemeinsamen Streuung}.\\
Formel: d$ = \frac{\bar{x}_1 - \bar{x}_2}{s_p}$ mit der gepoolten SD $s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}$
\textbf{Richtwerte nach Cohen:}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item $d \approx 0.2 \Rightarrow$ kleiner Effekt
  \item $d \approx 0.5 \Rightarrow$ mittlerer Effekt
  \item $d \approx 0.8 \Rightarrow$ grosser Effekt
\end{itemize}

\subsection{Odds $\&$ Odds Ratio}
Für Ja/Nein-Daten (z. B. Conversion, Risiko) brauchst du andere Effektgrössen.

\subsubsection{Odds}
= Chancenverhältnis\\
$\text{Odds} = \frac{P(\text{Ereignis})}{P(\text{kein Ereignis})}$

\subsubsection{Odds Ratio (OR)}
Odds Ratio vergleicht die Odds zweier Gruppen: $OR = \frac{\text{Odds}_1}{\text{Odds}_2}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item $OR = 1 \Rightarrow$ gleiche Chancen
  \item $OR > 1 \Rightarrow$ Gruppe 1 hat höhere Odds als Gruppe 2
  \item $OR < 1 \Rightarrow$ Gruppe 1 hat geringere Odds
\end{itemize}

\subsection{Risk Ratio (RR) $\&$ Odds Ratio bei Anteilen}
Bei A/B-Tests oder Risiken:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item Gruppe 1: Anteil $p_1$ (z.\,B. Risiko in der Impfgruppe)
  \item Gruppe 2: Anteil $p_2$ (z.\,B. Risiko in der Kontrollgruppe)
\end{itemize}

\subsubsection{Risk Ratio (RR)}
= Verhältnis der Risiken / Anteile\\
$RR = \frac{p_1}{p_2}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item $RR < 1 \Rightarrow$ Gruppe 1 hat \textbf{geringeres Risiko}
  \item $RR = 0.33 \Rightarrow$ Risiko reduziert sich auf 33\,\% des Vergleichsrisikos (oder: 67\,\% Reduktion).
\end{itemize}

\subsubsection{Odds Ratio (OR) bei Anteilen}
$OR = \frac{p_1/(1-p_1)}{p_2/(1-p_2)}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item RR ist intuitiver („halbes Risiko“)
    \item OR ist wichtig, wenn man mit Logistic Regression arbeitet oder bei sehr seltenen Ereignissen (RR $\approx$ OR)
\end{itemize}

\section{Gruppenvergleiche}

\subsection{ANOVA}
\textbf{Problem}: Mehr als zwei Gruppen (z.B. A, B, C) mit unterschiedlichen Mittelwerten.
\\Ein t-Test kann immer nur zwei Gruppen vergleichen. Viele t-Tests $\Rightarrow$ Fehlerinflation.\\
$\Rightarrow$ ANOVA prüft in einem Test, ob es globale Mittelwertunterschiede für 3+ Gruppen gibt.
 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/ANOVA.png}
\end{figure}

ANOVA vergleicht:
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item \textbf{Between-Varianz}
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Unterschiede der Gruppenmittelwerte
        \item Unterschiede der Gruppenmittelwerte
    \end{itemize}
    \item \textbf{Within-Varianz}
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Streuung \textbf{innerhalb der Gruppen}
        \item „Wie stark streuen die Punkte/Individuen um den Balken herum?“
    \end{itemize}
\end{enumerate}
\textbf{Teststatistik (Mean Square-Verhältnis)}: $F = \frac{MS_{\text{between}}}{MS_{\text{within}}}$
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item $MS_{\text{between}}$: mittlere Quadratsumme \textbf{zwischen} Gruppen
    \item $MS_{\text{within}}$: mittlere Quadratsumme \textbf{innerhalb} Gruppen
\end{itemize}
Hoher F $\rightarrow$ Gruppen unterscheiden sich stärker als zufällig erwartet

\subsubsection{Ablauf}
\begin{enumerate}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item Gruppenmittelwerte berechnen.
  \item Between-Varianz ($SS_{\text{between}}$) bestimmen
  \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Sum of Squares zwischen den Gruppen.
  \end{itemize}
  \item Within-Varianz ($SS_{\text{within}}$) bestimmen
  \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Sum of Squares innerhalb der Gruppen.
  \end{itemize}
  \item Mittelquadrate berechnen:\\
  $MS_{\text{between}} = \frac{SS_{\text{between}}}{df_{\text{between}}}$\\
  $MS_{\text{within}} = \frac{SS_{\text{within}}}{df_{\text{within}}}$
  \item F-Wert berechnen:\\
  $F = \frac{MS_{\text{between}}}{MS_{\text{within}}}$
  \item p-Wert aus der F-Verteilung mit passenden $df$ bestimmen.
\end{enumerate}
\begin{figure}[H]
      \centering
      \includegraphics[width=0.9\linewidth]{img/ANOVA_Ablauf.png}
\end{figure}
ANOVA testet eine globale Nullhypothese über alle Gruppenmittelwerte.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
  \item H$_0$: $\mu_1 = \mu_2 = \cdots = \mu_k$ (alle k Gruppen haben denselben Mittelwert)
  \item H$_1$: Mindestens eine Gruppe weicht ab
  \item ANOVA sagt \textbf{nicht}, welche Gruppe anders ist $\Rightarrow$ Dafür braucht es Post-hoc Tests
\end{itemize}

\subsection{Post-hoc Test}
\textbf{„Zwischen welchen Gruppen liegen Unterschiede und wie stark?“}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/PostHoc}
\end{figure}

\subsubsection{Post-hoc: Tukey HSD}
\textbf{HSD = Honest Significant Difference}\\
Tukey HSD ist der Standard-Post-hoc nach ANOVA:
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item vergleicht alle Gruppenpaare gleichzeitig
    \item nutzt die gemeinsame Varianzschätzung aus der ANOVA (MS\_within / gepoolte Varianz)
    \item liefert für jedes Paar:
    \begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
        \item Mittelwertdifferenz
        \item Konfidenzintervall
        \item Konfidenzintervall
    \end{itemize}
    \item robust, auch wenn es viele Paarvergleiche gibt
    \item kontrolliert Fehler 1. Art über alle Paarvergleiche zusammen
\end{itemize}
\textbf{ANOVA zeigt \large{ob}, Post-hoc zeigt \large{wo} Unterschiede liegen.}

\subsection{Kruskal-Wallis}
nichtparametrischer Test für 3+ unabhängige Gruppen, der nur mit Rängen arbeitet
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item funktioniert für metrische oder ordinale Daten
    \item braucht keine Normalverteilung
    \item testet v. a. Lage-/Medianunterschiede
    \item ist die Verallgemeinerung des Wilcoxon-Rangsummen-Tests von 2 auf k Gruppen.
\end{itemize}
Warum machen Ränge den Test robust?
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Extreme Ausreisser bekommen nur „grosse Ränge“, aber keinen quadratischen Einfluss wie in Varianz-Berechnungen
    \item Schiefe + heavy tails stören viel weniger, weil nur die Reihenfolge zählt, nicht die genauen Abstände
    \end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/Kruskal_Wallis.png}
\end{figure}
\textbf{Notation:}
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item $k = \text{Anzahl Gruppen}$
    \item $n_i = \text{Gruppengrösse i}$
    \item $N = \text{Gesamtzahl Beobachtungen} = \sum_i n_i$
    \item $R_i = \text{Summe der Ränge in Gruppe i}$
\end{itemize}
\textbf{Die Kruskal-Wallis-Statistik:}$ H = \frac{12}{N(N+1)} \sum_{i=1}^k \frac{R_i^2}{n_i} - 3(N+1)$\\
Unter $H_0$ (,,alle Gruppen stammen aus Populationen mit \textbf{gleicher Verteilung}’’):
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item folgt $H$ \textbf{approximativ einer $\chi^2$-Verteilung} mit $df = k - 1$ (bei ausreichend grossen $n_i$).
    \item p-Wert $= P(\chi^2_{k-1} \ge H_{\text{beobachtet}})$.
\end{itemize}
Wenn $p \le \alpha$ $\rightarrow$ Nullhypothese verwerfen $\rightarrow$ \textbf{mindestens eine Gruppe unterscheidet sich in der Lage/Verteilung}.

\subsection{Multiple Tests}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/Multiple_Test.png}
\end{figure}
Viele parallele Tests erhöhen die Wahrscheinlichkeit für Zufallstreﬀer massiv.\\
Wenn du m Tests machst, steigt die Chance auf mindestens einen Fehlalarm stark an:
\\Gesamtfehler$ = 1 - (1 - \alpha)^m$\\
$\rightarrow$ Ohne Korrektur erhält man viele scheinbare "Treffer" $\Rightarrow$ Verfahren zur Fehlerkontrolle, z. B.
\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item Bonferroni-Korrektur (sehr streng)
    \item FDR (False Discovery Rate)
\end{itemize}

\subsubsection{Bonferroni-Korrektur}
(zu streng, aber einfach)\\
\textbf{Ziel}: Gesamtfehler (family-wise error rate) $\le \alpha$ halten.\\
Wenn du $m$ Tests machst, gibst du jedem Test nur noch $\alpha_{\text{neu}} = \frac{\alpha}{m}$ als Signifikanzschwelle.\\
\textbf{Vorteil}: sehr starker Schutz vor Fehlalarmen $\rightarrow$ fast keine False Positives mehr.\\
\textbf{Nachteil}: extrem wenig Power $\rightarrow$ viele echte Effekte werden nicht mehr signifikant $\rightarrow$ „verschwinden“



\input{text/glossar}
\end{document}

$\Rightarrow$
$~~$\\

\begin{itemize}[itemsep=0.4em, topsep=0.4em, parsep=0em, labelsep=0.5em, leftmargin=*, align=parleft]
    \item 
    \item 
    \item 
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/B2B_B2C.png}
    \caption{}
    \label{fig:marktorientierung}
\end{figure}

\noindent
\begin{minipage}{0.48\textwidth}

\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
   
\end{minipage}